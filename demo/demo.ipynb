{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import model_crommelin_seasonal\n",
    "import feature_crommelin \n",
    "from importlib import reload\n",
    "import sys \n",
    "import os\n",
    "from os import mkdir, makedirs\n",
    "from os.path import join,exists\n",
    "from importlib import reload\n",
    "import pickle\n",
    "import helper2\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequence of flags specifies which cells to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_reanalysis_flag = 0\n",
    "create_hindcasts_flag =  0\n",
    "featurize_flag =         1\n",
    "cluster_flag =           1\n",
    "estimate_transmat_flag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create directories to save results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize the directory of data and features to be read from the data. \n",
    "topic_dir = \"/scratch/jf4241/crommelin\"\n",
    "filedict = dict({\n",
    "    \"data\": dict({\n",
    "        \"ra\": dict({\n",
    "            \"traj\": dict({\n",
    "                \"dir\": join(topic_dir, \"reanalysis\", \"trajectory\", \"2022-07-28\", \"0\"), # Directory with possibly many files\n",
    "            }),\n",
    "            \"feat_all\": dict({\n",
    "                \"dir\": join(topic_dir, \"reanalysis\", \"features_all\", \"2022-07-28\", \"0\"),\n",
    "                \"filename\": \"crom_feat_all.nc\",\n",
    "            }),\n",
    "            \"feat_tpt\": dict({\n",
    "                \"dir\": join(topic_dir, \"reanalysis\", \"features_tpt\", \"2022-07-28\", \"0\",),\n",
    "                \"filename\": \"crom_feat_tpt.nc\",\n",
    "            }),\n",
    "        }),\n",
    "        \"hc\": dict({\n",
    "            \"traj\": dict({\n",
    "                \"dir\": join(topic_dir, \"hindcast\", \"trajectory\", \"2022-07-28\", \"0\"), # Directory with possibly multiple files\n",
    "            }),\n",
    "            \"feat_all\": dict({\n",
    "                \"dir\": join(topic_dir, \"hindcast\", \"features_all\", \"2022-07-28\", \"0\"),\n",
    "                \"filename\": \"crom_feat_all.nc\",\n",
    "            }),\n",
    "            \"feat_tpt\": dict({\n",
    "                \"dir\": join(topic_dir, \"hindcast\", \"features_tpt\", \"2022-07-28\", \"0\"),\n",
    "                \"filename\": \"crom_feat_tpt.nc\",\n",
    "            })\n",
    "        }),        \n",
    "    }),\n",
    "    \"results\": dict({\n",
    "        \"dir\": join(topic_dir, \"results\", \"2022-07-28\", \"0\")\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directories if they don't exist already\n",
    "for src in [\"ra\",\"hc\"]:\n",
    "    for fmt in [\"traj\",\"feat_all\",\"feat_tpt\"]:\n",
    "        path = filedict[\"data\"][src][fmt][\"dir\"]\n",
    "        if not exists(path):\n",
    "            makedirs(path, exist_ok=True)\n",
    "path = filedict[\"results\"][\"dir\"]\n",
    "if not exists(path): makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set physical parameters for the dynamical model, including the size of the channel, the strength of drag and forcing and dissipation, and the length of the annual cycle. All this is stored in a \"fundamental parameter dictionary\" (FPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpd = dict({\n",
    "    \"b\": 0.5, \"beta\": 1.25, \"gamma_limits\": [0.15, 0.22], \n",
    "    \"C\": 0.1, \"x1star\": 0.95, \"r\": -0.801, \"year_length\": 400.0,\n",
    "})\n",
    "crom = model_crommelin_seasonal.SeasonalCrommelinModel(fpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set simulation parameters for the reanalysis (RA) simulation. We will start the simulation partway through year 1957 (remember, the years are 400 days long, so this doesn't correspond to our world) and save out every 0.5 days. We will start with a 500-day \"burn-in\" period to let the system settle onto its attractor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_samp = 0.5 # Time step to save out\n",
    "duration_burnin = 500.0 # Run 500 days of burn-in\n",
    "t_init_burnin = (1957 + 0.2)*fpd[\"year_length\"] # Start the burnin partway through 1957, in our pretend model world\n",
    "duration_ra = 200.0*fpd[\"year_length\"] # Run 200 years of reanalysis (abbreviated RA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model integration will be stored as a 3-dimensional array with shape `(Nx, Nt, xdim)`, where\n",
    "* `Nx` is the number of parallel ensemble members. \n",
    "* `Nt` is the number of timesteps.\n",
    "* `xdim` is the number of dimensions, or degrees of freedom, in the model. Here `xdim = 7` for `x_1,...,x_6,t`. \n",
    "\n",
    "I have added time-dependence to the model of Crommelin (2004) to make an explicitly seasonal event. For reanalysis, there is only one thread of history, so `Nx = 1`. Later, we will make hindcast data and have `Nx = 10`. We will initialize the integration with an array of shape `(Nx,xdim)` for the integrator. The integrator will automatically save out to a Netcdf file (`burnin.nc`), which will be opened afterward to initialize the long integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros((1,7))\n",
    "x0[0,-1] = t_init_burnin # Time is the last dimension\n",
    "t_save = np.arange(0, duration_burnin, dt_samp)\n",
    "traj_filename_burnin = join(filedict[\"data\"][\"ra\"][\"traj\"][\"dir\"], \"burnin.nc\")\n",
    "crom.integrate_and_save(x0,t_save,traj_filename_burnin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have let the model find its attractor, we set up a longer simulation starting from the end of the burn-in. Below we specify the filenames to save both the model integration and the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_burnin = xr.open_dataset(traj_filename_burnin)[\"X\"]\n",
    "x0 = x_burnin[:,-1].data # End of burn-in = beginning of reanalyis\n",
    "t_init_str,_ = crom.date_format(x0[0,-1])\n",
    "t_fin_str,_ = crom.date_format(x0[0,-1] + duration_ra)\n",
    "traj_filename_ra = join(filedict[\"data\"][\"ra\"][\"traj\"][\"dir\"], f\"ra_{t_init_str}_to_{t_fin_str}.nc\")\n",
    "param_filename_ra = join(filedict[\"data\"][\"ra\"][\"traj\"][\"dir\"], \"params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we finally create the reanalysis data. Hopefully we don't need to do this too many times, so a flag is there to prevent running this cell every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_reanalysis_flag:\n",
    "    t_save = np.arange(0,duration,dt_samp)\n",
    "    crom.integrate_and_save(x0,t_save,traj_filename_ra,metadata_filename=param_filename_ra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hindcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have reanalysis, we can make \"hindcast\" data by launching many perturbed parallel trajectories from reanalysis, to see alternative realities play out. The following parameters specify exactly which data to create:\n",
    "\n",
    "* `t_range_hc` says what time window the hindcasts will cover, in this case 1960-1970 (a strict subset of the reanalysis period)\n",
    "* `ens_size` is the size of each ensemble, in this case 10 (to match the S2S dataset).\n",
    "* `ens_gap` is the time separation between subsequent ensembles, in this case 13 days. (For S2S, it's 2 or 3 days).\n",
    "* `ens_duration` is the duration of the ensemble members, in this case 47 (to match the S2S dataset). \n",
    "* `pert_scale` is the size of random initial-condition perturbations. We choose `0.01` arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_hindcasts_flag:\n",
    "    t_range_hc = crom.q[\"year_length\"]*np.array([1960,1970])\n",
    "    crom.generate_hindcast_dataset(\n",
    "        [traj_filename_ra],filedict[\"data\"][\"hc\"][\"traj\"][\"dir\"],t_range_hc,dt_samp,\n",
    "        ens_size=10,ens_duration=47,ens_gap=13,pert_scale=0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of any data analysis routine is to extract features, or observables, of interest, which depend on the system at hand. The file `feature_template.py` defines an abstract class, `SeasonalFeatures`, for organizing features of a seasonal phenomenon into `xarray` datasets for easy access. (By \"seasonal phenomenon\", I mean some dynamics with a particular season of interest, e.g., winter The file `feature_crommelin.py` defines a subclass,  `SeasonalCrommelinModelFeatures`, which implements observables that are physically relevant to the simple blocking model. As the model is only `6+1`-dimensional, the feature space is actually larger than the model state space. For more realistic models, such as the ECMWF Integrated Forecast System (IFS) which produces hindcasts, the feature space will be a major *reduction* from the state space. The code below will specify which observables we want, as we read them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, reading features from the dataset will be expensive, as we have to comb through a large database of files, so we should minimize the number of times we do this in development. One strategy is to separate the featurization into multiple stages, with each stage repeated less often than the previous one. The first stage is to read *all* possible features we *might* use for the downstream tasks and save them to a file, with the understanding that we will take some further reductions downstream. The list of observables for now is\n",
    "* `identity`: the model state space\n",
    "* `energy`: the energy contained in each wavenumber, as well as the total\n",
    "* `energy_tendency`: the rate of change of total energy, and its decomposition into dissipation and forcing\n",
    "* `energy_exchange`: the rate of energy *flow* between different wavenumbers, as well as dissipation and forcing, separated by wavenumber.\n",
    "* `phase`: the phase of each wave (i.e., how it's shifted with respect to the zonal domain walls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "crom_feat = feature_crommelin.SeasonalCrommelinModelFeatures()\n",
    "qra = pickle.load(open(param_filename_ra,\"rb\")) # qra contains parameters of the Crommelin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if featurize_flag:\n",
    "    feat_all = dict()\n",
    "    for src in [\"ra\",\"hc\"]:\n",
    "        ds_feature_list = []\n",
    "        file_list = [f for f in os.listdir(filedict[\"data\"][src][\"traj\"][\"dir\"]) if (f.startswith(src) and f.endswith(\".nc\"))]\n",
    "        for filename in file_list:\n",
    "            ds = xr.open_dataset(join(filedict[\"data\"][src][\"traj\"][\"dir\"], filename))\n",
    "            ds_feature_list += [\n",
    "                xr.Dataset(\n",
    "                    data_vars = dict({\n",
    "                        # WARNING! So that this steps work, the 'feature' coordinate of each observable must be uniquely named. \n",
    "                        # For example, if the identity observable and energy observable both have a coordinate called \"feature\", \n",
    "                        # then xarray will assume it's the same and smush them together, filling in NaNs. This restricts what \n",
    "                        # we can use as feature names. \n",
    "                        \"identity\": crom_feat.identity_observable(ds, qra),\n",
    "                        \"energy\": crom_feat.energy_observable(ds, qra),\n",
    "                        \"energy_tendency\": crom_feat.energy_tendency_observable(ds, qra),\n",
    "                        \"energy_exchange\": crom_feat.energy_exchange_observable(ds, qra),\n",
    "                        \"phase\": crom_feat.phase_observable(ds, qra),\n",
    "                    })\n",
    "                )\n",
    "            ]\n",
    "            ds.close()\n",
    "        feat_all[src] = xr.concat(ds_feature_list,dim=\"ensemble\") # ensemble is a new dimension\n",
    "        feat_all[src].to_netcdf(join(filedict[\"data\"][src][\"feat_all\"][\"dir\"], filedict[\"data\"][src][\"feat_all\"][\"filename\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd = dict(\n",
    "    dt_szn = 5.0,\n",
    "    szn_start = 300.0,\n",
    "    szn_length = 250.0,\n",
    "    year_length = 400.0,\n",
    "    szn_avg_window = 5.0,\n",
    ")\n",
    "epd[\"Nt_szn\"] = int(epd[\"szn_length\"] / epd[\"dt_szn\"])\n",
    "crom_feat.set_event_params(epd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset observables for use in TPT analysis\n",
    "Incorporate any time-delay information we might want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_all = dict()\n",
    "feat_tpt = dict()\n",
    "# List the features to put into feat_tpt\n",
    "# First, the features needed to define A and B: the time, the x1 coordinate, and its running mean, min, and max\n",
    "# over some time horizon. \n",
    "time_horizon = 40.0 # time units\n",
    "feat_tpt_list = ([\n",
    "    \"t_abs\",\"x1_runmean\",\"x1_runmin\",\"x1_runmax\",\n",
    "    \"t_szn\",\"szn_start_year\",\"t_cal\",\"ti_szn\"\n",
    "])\n",
    "# Physical features\n",
    "feat_tpt_list += ([\n",
    "    \"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\n",
    "    \"E_01\",\"E_02\",\"E_11\",\"E_12\",\"E_tot\",\n",
    "    \"Edot_dissipation\",\"Edot_forcing\",\n",
    "    \"Ex_11-diss\", \"Ex_12-diss\", \"Ex_11-02\", \"Ex_11-12\",\n",
    "    \"ph_11\",\"ph_12\",\n",
    "])\n",
    "for src in [\"ra\",\"hc\"]:\n",
    "    feat_all[src] = xr.open_dataset(join(filedict[\"data\"][src][\"feat_all\"][\"dir\"], filedict[\"data\"][src][\"feat_all\"][\"filename\"]))\n",
    "    feat_tpt[src] = xr.Dataset(\n",
    "        data_vars = {\n",
    "            \"X\": xr.DataArray(               \n",
    "                coords = {\n",
    "                    \"ensemble\": feat_all[src].coords[\"ensemble\"],\n",
    "                    \"member\": feat_all[src].coords[\"member\"],\n",
    "                    \"t_sim\": feat_all[src].coords[\"t_sim\"],\n",
    "                    \"feature\": feat_tpt_list,\n",
    "                },\n",
    "                data = np.zeros((\n",
    "                    feat_all[src][\"ensemble\"].size, feat_all[src][\"member\"].size, \n",
    "                    feat_all[src][\"t_sim\"].size, len(feat_tpt_list)\n",
    "                )),\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    for id_coord in [\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\"]:\n",
    "        feat_tpt[src][\"X\"].loc[dict(feature=id_coord)] = (\n",
    "            feat_all[src][\"identity\"].sel(feature=id_coord)\n",
    "        )\n",
    "    for e_coord in [\"E_01\",\"E_02\",\"E_11\",\"E_12\",\"E_tot\"]:\n",
    "        feat_tpt[src][\"X\"].loc[dict(feature=e_coord)] = (\n",
    "            feat_all[src][\"energy\"].sel(reservoir=e_coord.replace(\"_\",\"\"))\n",
    "        )\n",
    "    for ph_coord in [\"ph_11\",\"ph_12\"]:\n",
    "        feat_tpt[src][\"X\"].loc[dict(feature=ph_coord)] = (\n",
    "            feat_all[src][\"phase\"].sel(wavenumber=ph_coord.replace(\"_\",\"\"))\n",
    "        )\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"Ex_11-diss\")] = (\n",
    "        feat_all[src][\"energy_exchange\"].sel(source=\"E11\",sink=\"dissipation\"))\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"Ex_12-diss\")] = (\n",
    "        feat_all[src][\"energy_exchange\"].sel(source=\"E12\",sink=\"dissipation\"))\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"Ex_11-02\")] = (\n",
    "        feat_all[src][\"energy_exchange\"].sel(source=\"E11\",sink=\"E02\"))\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"Ex_11-12\")] = (\n",
    "        feat_all[src][\"energy_exchange\"].sel(source=\"E11\",sink=\"E12\"))\n",
    "\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"t_abs\")] = feat_all[src][\"identity\"].sel(feature=\"t_abs\")\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"Edot_dissipation\")] = feat_all[src][\"energy_tendency\"].sel(Eflow=\"dissipation\")\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"Edot_forcing\")] = feat_all[src][\"energy_tendency\"].sel(Eflow=\"forcing\")    \n",
    "    # For the A- and B-defining feature of x1, take the running mean, minimum, and maximum of x1 over the time horizon\n",
    "    dt_samp = (feat_all[src][\"t_sim\"][1] - feat_all[src][\"t_sim\"][0]).data # Assume uniform time sampling!\n",
    "    num_delays = int(time_horizon/dt_samp) + 1\n",
    "    rolling_x1 = feat_all[src][\"identity\"].sel(feature=\"x1\").rolling(\n",
    "        dim={\"t_sim\": num_delays}, min_periods=num_delays) \n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"x1_runmean\")] = rolling_x1.mean() \n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"x1_runmin\")] = rolling_x1.min() \n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"x1_runmax\")] = rolling_x1.max() \n",
    "    # Compute the calendar time and other temporal metadata, as this defines the seasonality \n",
    "    szn_start_year,t_cal,t_szn,ti_szn = crom_feat.time_conversion_from_absolute(\n",
    "        feat_all[src][\"identity\"].sel(feature=\"t_abs\")\n",
    "    )\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"t_szn\")] = t_szn # Time since the most recent season beginning\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"szn_start_year\")] = szn_start_year\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"t_cal\")] = t_cal\n",
    "    feat_tpt[src][\"X\"].loc[dict(feature=\"ti_szn\")] = ti_szn # Which window of the discretized season this sample belongs to\n",
    "    # Save \n",
    "    feat_tpt[src].to_netcdf(join(filedict[\"data\"][src][\"feat_tpt\"][\"dir\"], filedict[\"data\"][src][\"feat_tpt\"][\"filename\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(helper2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot seasonal statistics of an observable\n",
    "feat = \"t_szn\"\n",
    "obs = \"E_01\"\n",
    "sel = dict(ensemble=0, member=0)\n",
    "fig,ax = helper2.plot_field_1d(\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(sel,drop=True).sel(feature=obs).data.flatten(),\n",
    "    np.ones(feat_tpt[\"ra\"][\"t_sim\"].size),\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(sel).sel(feature=feat).data.flatten(),\n",
    "    feat_name=\"Time into season\", field_name=obs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining A and B\n",
    "First we make methods for computing the hitting times and destinations (both in forward and backward time) for two sets A and B, as well as their complement D. Each method will have an additional argument, \"tpt_bndy\", a dictionary of parameters (e.g., thresholds) that specify A and B. This will allow us to consider a whole parameterized family of A and B later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: a function to assign each data point to A, B, or D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcode = {\"A\": 0, \"B\": 1, \"D\": 2}\n",
    "def abtest(Xtpt, tpt_bndy):\n",
    "    # Given a snapshot of an instance of the feat_tpt data structure, return ab_tag: \n",
    "    # 0 means in A, 1 means in B, and 2 means neither.\n",
    "    # The definition of A and B will be parameterized by a dictionary, tpt_bndy, which specifies\n",
    "    # the time of season when blockings can happen as well as the thresholds for A and B. \n",
    "    time_window_flag = 1.0*(\n",
    "        Xtpt.sel(feature=\"t_szn\") >= tpt_bndy[\"tthresh\"][0])*(\n",
    "        Xtpt.sel(feature=\"t_szn\") <= tpt_bndy[\"tthresh\"][1]\n",
    "    )\n",
    "    blocked_flag = 1.0*(Xtpt.sel(feature=\"x1\") <= tpt_bndy[\"x1thresh\"][0])\n",
    "    zonal_flag = 1.0*(Xtpt.sel(feature=\"x1\") >= tpt_bndy[\"x1thresh\"][1])\n",
    "    ab_tag = (\n",
    "        abcode[\"A\"]*((1*(time_window_flag == 0) + 1*zonal_flag) > 0) + \n",
    "        abcode[\"B\"]*(time_window_flag*blocked_flag) + \n",
    "        abcode[\"D\"]*(time_window_flag*(blocked_flag==0)*(zonal_flag==0))\n",
    "    )\n",
    "    return ab_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second: compute hitting times. The optimal way to do this depends on whether we have a small number of long trajectories (as in reanalysis), or a large number of short trajectories (as in hindcasts). The functions are named \"Cotton Eye Joe\", a reference to the American folk song with the famous lyric \"where did you come from, where did you go?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the time since and until hitting A and B\n",
    "def cotton_eye_joe_timesteps(Xtpt, tpt_bndy, ab_tag):\n",
    "    sintil = xr.DataArray(\n",
    "        coords = dict({\n",
    "            \"ensemble\": Xtpt.coords[\"ensemble\"],\n",
    "            \"member\": Xtpt.coords[\"member\"],\n",
    "            \"t_sim\": Xtpt.coords[\"t_sim\"],\n",
    "            \"sense\": [\"since\",\"until\"],\n",
    "            \"state\": [\"A\",\"B\"]\n",
    "        }),\n",
    "        data = np.nan*np.ones((Xtpt[\"ensemble\"].size, Xtpt[\"member\"].size, Xtpt[\"t_sim\"].size, 2, 2)),\n",
    "        dims = [\"ensemble\",\"member\",\"t_sim\",\"sense\",\"state\"],\n",
    "    )\n",
    "    # Forward pass through time \n",
    "    for i_time in np.arange(sintil[\"t_sim\"].size):    \n",
    "        if i_time % 200 == 0:\n",
    "            print(f\"Forward pass: through time {i_time} out of {sintil['t_sim'].size}\")\n",
    "        for state in [\"A\",\"B\"]:            \n",
    "            if i_time > 0:\n",
    "                sintil[dict(t_sim=i_time)].loc[dict(sense=\"since\",state=state)] = (\n",
    "                    sintil.isel(t_sim=i_time-1).sel(sense=\"since\",state=state).data + \n",
    "                    sintil[\"t_sim\"][i_time].data - sintil[\"t_sim\"][i_time-1].data\n",
    "                )\n",
    "            state_flag = (ab_tag.isel(t_sim=i_time) == abcode[state])\n",
    "            # Wherever the state is achieved at this time slice, set the time since to zero\n",
    "            sintil[dict(t_sim=i_time)].loc[dict(sense=\"since\",state=state)] = (\n",
    "                (xr.zeros_like(sintil.isel(t_sim=i_time).sel(sense=\"since\",state=state))).where(\n",
    "                state_flag, sintil.isel(t_sim=i_time).sel(sense=\"since\",state=state))\n",
    "            )\n",
    "    # Backward pass through time\n",
    "    for i_time in np.arange(sintil[\"t_sim\"].size-1,-1,-1):\n",
    "        if i_time % 200 == 0:\n",
    "            print(f\"Backward pass: through time {i_time} out of {sintil['t_sim'].size}\")\n",
    "        for state in [\"A\",\"B\"]:\n",
    "            if i_time < sintil[\"t_sim\"].size-1:\n",
    "                sintil[dict(t_sim=i_time)].loc[dict(sense=\"until\",state=state)] = (\n",
    "                    sintil.isel(t_sim=i_time+1).sel(sense=\"until\",state=state).data + \n",
    "                    sintil[\"t_sim\"][i_time+1].data - sintil[\"t_sim\"][i_time].data\n",
    "                )\n",
    "            state_flag = (ab_tag.isel(t_sim=i_time) == abcode[state])\n",
    "            sintil[dict(t_sim=i_time)].loc[dict(sense=\"until\",state=state)] = (\n",
    "                (xr.zeros_like(sintil.isel(t_sim=i_time).sel(sense=\"until\",state=state))).where(\n",
    "                state_flag, sintil.isel(t_sim=i_time).sel(sense=\"until\",state=state))\n",
    "            )\n",
    "    return sintil   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the time since and until hitting A and B\n",
    "def cotton_eye_joe_timechunks(Xtpt, tpt_bndy, ab_tag):\n",
    "    sintil = xr.DataArray(\n",
    "        coords = dict({\n",
    "            \"ensemble\": Xtpt.coords[\"ensemble\"],\n",
    "            \"member\": Xtpt.coords[\"member\"],\n",
    "            \"t_sim\": Xtpt.coords[\"t_sim\"],\n",
    "            \"sense\": [\"since\",\"until\"],\n",
    "            \"state\": [\"A\",\"B\"]\n",
    "        }),\n",
    "        data = np.nan*np.ones((Xtpt[\"ensemble\"].size, Xtpt[\"member\"].size, Xtpt[\"t_sim\"].size, 2, 2)),\n",
    "        dims = [\"ensemble\",\"member\",\"t_sim\",\"sense\",\"state\"],\n",
    "    )\n",
    "    t_sim = Xtpt[\"t_sim\"].data\n",
    "    print(f\"t_sim.shape = {t_sim.shape}\")\n",
    "    Nt = t_sim.size\n",
    "    # Forward pass through time \n",
    "    for ensemble in Xtpt.coords[\"ensemble\"]:\n",
    "        for member in Xtpt.coords[\"member\"]:\n",
    "            for state in [\"A\",\"B\"]:\n",
    "                indicator = (ab_tag.sel(ensemble=ensemble,member=member) == abcode[state]).data.astype(int)\n",
    "                tsince = np.nan*np.ones(Nt)\n",
    "                tuntil = np.nan*np.ones(Nt)\n",
    "                # Fill in zeros inside the set\n",
    "                tsince[indicator==1] = 0.0\n",
    "                tuntil[indicator==1] = 0.0\n",
    "                # Find the crossover points\n",
    "                idx_exit = np.where(np.diff(indicator) == -1)[0] + 1 # First step outside of state\n",
    "                idx_entry = np.where(np.diff(indicator) == 1)[0] + 1 # First entry to state\n",
    "                # Take care of boundary cases\n",
    "                if (not indicator[0]) and len(idx_entry) > 0:\n",
    "                    tuntil[:idx_entry[0]] = t_sim[idx_entry[0]] - t_sim[:idx_entry[0]]\n",
    "                    idx_entry = idx_entry[1:]\n",
    "                if (not indicator[Nt-1]) and len(idx_exit) > 0:\n",
    "                    tsince[idx_exit[-1]:] = t_sim[idx_exit[-1]:] - t_sim[idx_exit[-1]-1]\n",
    "                    idx_exit = idx_exit[:-1]\n",
    "                # Now the middle components: time intervals between exits and entries\n",
    "                if len(idx_entry) > 0 and len(idx_exit) > 0:\n",
    "                    for k in range(len(idx_exit)):\n",
    "                        i0,i1 = idx_exit[k],idx_entry[k]\n",
    "                        tsince[i0:i1] = t_sim[i0:i1] - t_sim[i0-1]\n",
    "                        tuntil[i0:i1] = t_sim[i1] - t_sim[i0:i1]\n",
    "                sintil.loc[dict(ensemble=ensemble,member=member,state=state,sense=\"since\")] = tsince\n",
    "                sintil.loc[dict(ensemble=ensemble,member=member,state=state,sense=\"until\")] = tuntil\n",
    "    return sintil   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cotton_eye_joe(Xtpt, tpt_bndy, ab_tag, mode):\n",
    "    if mode == \"timechunks\":\n",
    "        return cotton_eye_joe_timechunks(Xtpt, tpt_bndy, ab_tag)\n",
    "    elif mode == \"timesteps\": \n",
    "        return cotton_eye_joe_timesteps(Xtpt, tpt_bndy, ab_tag)\n",
    "    else:\n",
    "        raise Exception(f\"You asked for a mode of {mode}, but I only accept 'timechunks' or 'timesteps'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a specific A and B.\n",
    "The steps below will probably be repeated often with different thresholds. `tthresh` is an array of two time thresholds: the earliest and latest times after the start of the blocking season that a block may occur. `x1thresh` is an array of two thresholds for blocking: the upper limit of B for `x1_runmax`, and the lower limit of A for `x1_runmin`. We will compute the tags for A and B, the hitting times, and the committors, all for reanalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpt_bndy = {\"tthresh\": [10,200], \"x1thresh\": [0.75, 2.0]}\n",
    "pickle.dump(tpt_bndy, open(join(filedict[\"results\"][\"dir\"], \"tpt_bndy\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in [\"ra\",\"hc\"]:\n",
    "    ab_tag[src].close()\n",
    "    sintil[src].close()\n",
    "    comm[src].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    ab_tag = dict()\n",
    "    sintil = dict() # Hitting times (since and until)\n",
    "    comm = dict() # Committor: to B in forward time, from A in backward time\n",
    "    for src in [\"ra\",\"hc\"]:\n",
    "        ab_tag[src] = abtest(feat_tpt[src][\"X\"], tpt_bndy)\n",
    "        mode = \"timechunks\" if src == \"ra\" else \"timesteps\"\n",
    "        sintil[src] = cotton_eye_joe(feat_tpt[src][\"X\"],tpt_bndy,ab_tag[src],mode=mode)\n",
    "        comm[src] = 1*(sintil[src].sel(state=\"B\") < sintil[src].sel(state=\"A\"))\n",
    "        comm[src].loc[dict(sense=\"since\")] = 1 - comm[src].sel(sense=\"since\")\n",
    "        # Save each item\n",
    "        ab_tag[src].to_netcdf(join(filedict[\"results\"][\"dir\"], f\"ab_tag_{src}.nc\"))    \n",
    "        sintil[src].to_netcdf(join(filedict[\"results\"][\"dir\"], f\"sintil_{src}.nc\"))    \n",
    "        comm[src].to_netcdf(join(filedict[\"results\"][\"dir\"], f\"comm_{src}.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload these\n",
    "tpt_bndy = pickle.load(open(join(filedict[\"results\"][\"dir\"], \"tpt_bndy\"), \"rb\"))\n",
    "ab_tag = dict()\n",
    "sintil = dict() # Hitting times (since and until)\n",
    "comm = dict() # Committor: to B in forward time, from A in backward time\n",
    "for src in [\"ra\",\"hc\"]:\n",
    "    ab_tag[src] = xr.open_dataarray(join(filedict[\"results\"][\"dir\"], f\"ab_tag_{src}.nc\"))    \n",
    "    sintil[src] = xr.open_dataarray(join(filedict[\"results\"][\"dir\"], f\"sintil_{src}.nc\"))    \n",
    "    comm[src] = xr.open_dataarray(join(filedict[\"results\"][\"dir\"], f\"comm_{src}.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the A-B tag and hitting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(nrows=2, figsize=(10,15), sharex=True)\n",
    "sel = dict(member=0, t_sim=slice(None,6000))\n",
    "h_x1, = xr.plot.plot(\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(sel,drop=True).sel(feature=\"x1\"),\n",
    "    color=\"black\",ax=axes[0],label=\"$x_1$\",x=\"t_sim\"\n",
    ")\n",
    "h_x1_runmax, = xr.plot.plot(\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(sel,drop=True).sel(feature=\"x1_runmax\"),\n",
    "    color=\"gray\",ax=axes[0],label=\"$x_1$ running max\",x=\"t_sim\")\n",
    "# Find all the intervals in B\n",
    "inb_starts = np.where(np.diff(1*(ab_tag[\"ra\"].sel(sel,drop=True).data.flatten()==abcode[\"B\"])) == 1)[0]\n",
    "inb_ends = np.where(np.diff(1*(ab_tag[\"ra\"].sel(sel,drop=True).data.flatten()==abcode[\"B\"])) == -1)[0]\n",
    "print(f\"Before snips, B: {len(inb_starts)} starts, {len(inb_ends)} ends\")\n",
    "if inb_starts[0] > inb_ends[0]: \n",
    "    inb_starts = inb_starts[1:]\n",
    "if inb_starts[-1] > inb_ends[-1]:\n",
    "    inb_starts = inb_starts[:-1]\n",
    "print(f\"After snips, B: {len(inb_starts)} starts, {len(inb_ends)} ends\")\n",
    "for i in range(len(inb_starts)):\n",
    "    axes[0].axvspan(\n",
    "        feat_tpt[\"ra\"][\"t_sim\"][inb_starts[i]], feat_tpt[\"ra\"][\"t_sim\"][inb_ends[i]], \n",
    "        color='red', alpha=0.5, zorder=-1\n",
    "    )\n",
    "ina_starts = np.sort(\n",
    "    np.where(np.diff(1*(ab_tag[\"ra\"].sel(sel,drop=True).data.flatten()==abcode[\"A\"])) == 1)[0])\n",
    "ina_ends = np.sort(\n",
    "    np.where(np.diff(1*(ab_tag[\"ra\"].sel(sel,drop=True).data.flatten()==abcode[\"A\"])) == -1)[0])\n",
    "if ina_starts[0] > ina_ends[0]: \n",
    "    ina_ends = ina_ends[1:]\n",
    "if ina_starts[-1] > ina_ends[-1]:\n",
    "    ina_starts = ina_starts[:-1]\n",
    "for i in range(len(ina_starts)):\n",
    "    axes[0].axvspan(\n",
    "        feat_tpt[\"ra\"][\"t_sim\"][ina_starts[i]], feat_tpt[\"ra\"][\"t_sim\"][ina_ends[i]], \n",
    "        color='dodgerblue', alpha=0.5, zorder=-1\n",
    "    )    \n",
    "\n",
    "axes[0].axhline(y=tpt_bndy[\"x1thresh\"][0], color='black')\n",
    "axes[0].legend(handles=[h_x1_runmax,h_x1])\n",
    "h_tbp, = xr.plot.plot(\n",
    "    sintil[\"ra\"].sel(sel,drop=True).sel(state=\"B\", sense=\"until\", drop=True),\n",
    "    ax=axes[1], label=\"$\\\\tau_B^+$\", color=\"red\")\n",
    "h_tap, = xr.plot.plot(\n",
    "    sintil[\"ra\"].sel(sel,drop=True).sel(state=\"A\", sense=\"until\", drop=True),\n",
    "    ax=axes[1], label=\"$\\\\tau_A^+$\", color=\"dodgerblue\")\n",
    "axes[1].legend(handles=[h_tbp,h_tap])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and plot the committor, lead time, and current from reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the lead time as a function of E01 and E12\n",
    "reload(helper2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = \"Ex_11-02\"\n",
    "sel = dict(ensemble=0,member=0)\n",
    "cond = (\n",
    "    (feat_tpt[\"ra\"][\"X\"].sel(sel,drop=True).sel(feature=\"t_szn\") > tpt_bndy[\"tthresh\"][0]) * \n",
    "    (feat_tpt[\"ra\"][\"X\"].sel(sel,drop=True).sel(feature=\"t_szn\") < tpt_bndy[\"tthresh\"][0]+10) \n",
    ")\n",
    "helper2.plot_field_1d(\n",
    "    sintil[\"ra\"].sel(sel,drop=True).sel(sense=\"until\",state=\"B\").where(cond).data.flatten(), \n",
    "    np.ones(cond[\"t_sim\"].size),\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(sel).sel(feature=[feat]).where(cond).data.flatten(),\n",
    "    ax=axes[0],\n",
    "    feat_name=feat, field_name=\"Lead time to B\",\n",
    ")\n",
    "helper2.plot_field_1d(\n",
    "    comm[\"ra\"].sel(sel,drop=True).sel(sense=\"until\").where(cond).data.flatten(), \n",
    "    np.ones(cond[\"t_sim\"].size),\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(sel).sel(feature=feat).where(cond).data.flatten(),\n",
    "    ax=axes[1],\n",
    "    feat_name=feat, field_name=\"Committor to B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose two features for the space to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the lead time as a function of E01 and E12\n",
    "reload(helper2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat0,feat1 = \"Ex_11-diss\",\"Ex_11-12\"\n",
    "sel = dict(ensemble=0,member=0) \n",
    "cond = (\n",
    "    (feat_tpt[\"ra\"][\"X\"].sel(sel,drop=True).sel(feature=\"t_szn\") > tpt_bndy[\"tthresh\"][0]) * \n",
    "    (feat_tpt[\"ra\"][\"X\"].sel(sel,drop=True).sel(feature=\"t_szn\") < tpt_bndy[\"tthresh\"][1]) \n",
    ")\n",
    "qp = comm[\"ra\"].sel(sel,drop=True).sel(sense=\"until\").where(cond).data.flatten()\n",
    "Tb = sintil[\"ra\"].sel(sel,drop=True).sel(sense=\"until\",state=\"B\").where(cond).data.flatten()\n",
    "\n",
    "fig,axes = plt.subplots(ncols=3,figsize=(15,5), sharey=True, sharex=True)\n",
    "helper2.plot_field_2d(\n",
    "    qp,\n",
    "    np.ones(feat_tpt[\"ra\"][\"t_sim\"].size),\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(feature=[feat0,feat1]).where(cond).sel(sel,drop=True),\n",
    "    feat_names = [feat0,feat1], \n",
    "    shp=[25,25], fig=fig, ax=axes[0], field_name=r\"$q_B^+$\"\n",
    ")\n",
    "helper2.plot_field_2d(\n",
    "    -Tb*qp ,\n",
    "    qp,\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(feature=[feat0,feat1]).where(cond).sel(sel,drop=True),\n",
    "    feat_names = [feat0,feat1], \n",
    "    shp=[25,25], fig=fig, ax=axes[1], field_name=r\"$-\\eta_B^+$\"\n",
    ")\n",
    "helper2.plot_field_2d(\n",
    "    -Tb,\n",
    "    np.ones(feat_tpt[\"ra\"][\"t_sim\"].size),\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(feature=[feat0,feat1]).where(cond).sel(sel,drop=True),\n",
    "    feat_names = [feat0,feat1], \n",
    "    shp=[25,25], fig=fig, ax=axes[2], field_name=r\"$-\\tau_B^+$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov State Modeling using hindcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to building a MSM:\n",
    "1. Select a set of features\n",
    "2. Cluster the data *separately at each time of the season*\n",
    "3. Count transitions from each box to each other box\n",
    "4. Build a list of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(helper2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msm_features = [\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\"]\n",
    "feat_msm = dict()\n",
    "for src in [\"ra\",\"hc\"]:\n",
    "    feat_msm[src] = feat_tpt[src][\"X\"].sel(feature=msm_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a homogeneously shaped dataset for clustering. Therefore we need to de-mean and normalize each feature. Compute the mean and standard deviation (from reanalysis) for each time window throughout the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute seasonal statistics\n",
    "szn_stats_dict,edges,centers = helper2.project_field(\n",
    "    feat_msm[\"ra\"].sel(ensemble=0,member=0).data, \n",
    "    np.ones_like(feat_msm[\"ra\"].sel(ensemble=0,member=0).data),\n",
    "    feat_tpt[\"ra\"][\"X\"].sel(feature=\"t_szn\",ensemble=0,member=0).data.reshape(-1,1),\n",
    "    bounds = np.array([0.0, crom_feat.year_length+0.0001]).reshape(-1,1),\n",
    "    shp = (int(crom_feat.year_length/crom_feat.dt_szn + 0.5),)\n",
    ")\n",
    "szn_stats = xr.Dataset(\n",
    "    data_vars = dict({\n",
    "        key: xr.DataArray(\n",
    "            coords={\"t_szn_cent\": centers[0], \"feature\": msm_features,},\n",
    "            data=szn_stats_dict[key],\n",
    "            dims=[\"t_szn_cent\", \"feature\"],\n",
    "        ) \n",
    "        for key in list(szn_stats_dict.keys())\n",
    "    }),\n",
    "    attrs = {\n",
    "        \"t_szn_edge\": edges[0], \"t_szn_cent\": centers[0], \n",
    "        \"dt_szn\": edges[0][1]-edges[0][0], \"Nt_szn\": len(centers[0]),\n",
    "        \"szn_start\": epd[\"szn_start\"], \"szn_length\": epd[\"szn_length\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_msm_normalized = dict()\n",
    "szn_window = dict()\n",
    "szn_start_year = dict()\n",
    "traj_beginning_flag = dict() # 1 if the sample is in the first seasonal time window where the trajectory started\n",
    "traj_ending_flag = dict() # 1 if the sample is in the last seasonal time window occupied by the trajectory\n",
    "for src in [\"ra\",\"hc\"]:\n",
    "    feat_msm_normalized[src] = feat_msm[src].copy(deep=True)\n",
    "    szn_window[src] = (feat_tpt[src][\"X\"].sel(feature=\"t_szn\")/szn_stats.attrs[\"dt_szn\"]).astype(int)\n",
    "    szn_start_year[src] = feat_tpt[src][\"X\"].sel(feature=\"szn_start_year\").astype(int)\n",
    "    for i_win in range(szn_stats.attrs[\"Nt_szn\"]):\n",
    "        feat_msm_normalized[src] = xr.where(\n",
    "            szn_window[src]==i_win, \n",
    "            (feat_msm_normalized[src] - szn_stats[\"mean\"].isel(t_szn_cent=i_win,drop=True)) / szn_stats[\"std\"].isel(t_szn_cent=i_win,drop=True), \n",
    "            feat_msm_normalized[src]\n",
    "        )\n",
    "    # --------------- Mark the trajectories that originated in an earlier time window and will reach another time window ---------------\n",
    "    # TODO: correct these definitions for multi-year trajectories that end in the same place as they started\n",
    "    traj_ending_flag[src] = (\n",
    "        (szn_window[src] == szn_window[src].isel(t_sim=-1,drop=True))*\n",
    "        (szn_start_year[src] == szn_start_year[src].isel(t_sim=-1,drop=True))\n",
    "    )\n",
    "    traj_beginning_flag[src] = (\n",
    "        (szn_window[src] == szn_window[src].isel(t_sim=0,drop=True))*\n",
    "        (szn_start_year[src] == szn_start_year[src].isel(t_sim=0,drop=True))\n",
    "    )\n",
    "    # -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cluster into boxes at each timestep. Note, this could be parallelized. We will choose a subset of all the samples to find the clusters, and only those trajectories that neither start nor end in that time window. This way, we ensure no transition matrix rows are empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_seed = 43\n",
    "\n",
    "km_assignment = dict()\n",
    "km_centers = dict()\n",
    "km_n_clusters = dict()\n",
    "for src in [\"hc\",\"ra\"]:\n",
    "    km_assignment_src = -np.ones((feat_tpt[src][\"ensemble\"].size,feat_tpt[src][\"member\"].size,feat_tpt[src][\"t_sim\"].size), dtype=int)\n",
    "    km_centers[src] = []\n",
    "    km_n_clusters[src] = -np.ones(szn_stats.attrs[\"Nt_szn\"], dtype=int)\n",
    "\n",
    "    for i_win in range(szn_stats.attrs[\"Nt_szn\"]):\n",
    "        if i_win % 10 == 0:\n",
    "            print(f\"Starting K-means number {i_win} out of {szn_stats.attrs['Nt_szn']}\")\n",
    "        idx_in_window = np.where(szn_window[src].data==i_win) # All the data in this time window\n",
    "        # idx_for_clustering is all the data that we're allowed to use to build the KMeans object\n",
    "        if i_win == 0:\n",
    "            idx_for_clustering = np.where(\n",
    "                (szn_window[src].data==i_win)*\n",
    "                (traj_ending_flag[src].data == 0)\n",
    "            )\n",
    "        elif i_win == szn_stats.attrs[\"Nt_szn\"]-1:\n",
    "            idx_for_clustering = np.where(\n",
    "                (szn_window[src].data==i_win)*\n",
    "                (traj_beginning_flag[src].data == 0)\n",
    "            )            \n",
    "        else:\n",
    "            idx_for_clustering = np.where(\n",
    "                (szn_window[src].data==i_win)*\n",
    "                (traj_ending_flag[src].data == 0)*\n",
    "                (traj_beginning_flag[src].data == 0)\n",
    "            )\n",
    "        km_n_clusters[src][i_win] = min(200,max(1,len(idx_for_clustering[0]//2)))\n",
    "        km = KMeans(n_clusters=km_n_clusters[src][i_win],random_state=km_seed).fit(\n",
    "                feat_msm_normalized[src].data[idx_for_clustering])\n",
    "        km_assignment_src[idx_in_window] = km.predict(feat_msm_normalized[src].data[idx_in_window]) \n",
    "        km_centers[src] += [km.cluster_centers_]\n",
    "        #print(f\"\\tkm_centers computation: {t5 - t4}\")\n",
    "    km_assignment[src] = xr.DataArray(\n",
    "        coords={\"ensemble\": feat_tpt[src][\"ensemble\"], \"member\": feat_tpt[src][\"member\"], \"t_sim\": feat_tpt[src][\"t_sim\"]},\n",
    "        dims=[\"ensemble\",\"member\",\"t_sim\"],\n",
    "        data=km_assignment_src.copy()    #np.zeros((feat_tpt[src][\"ensemble\"].size,feat_tpt[src][\"member\"].size,feat_tpt[src][\"t_sim\"].size), dtype=int)\n",
    "    )    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the Markov State Model by counting transitions from box to box. This part could also be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_list = dict()\n",
    "for src in [\"hc\",\"ra\"]:\n",
    "    time_dim = list(szn_window[src].dims).index(\"t_sim\")\n",
    "    nontime_dims = np.setdiff1d(np.arange(len(szn_window[src].dims)), [time_dim])\n",
    "    P_list[src] = []\n",
    "    for i_win in range(szn_stats.attrs[\"Nt_szn\"]-1):\n",
    "        if i_win % 10 == 0: print(f\"i_win = {i_win}\")\n",
    "        P = np.zeros((km_n_clusters[src][i_win],km_n_clusters[src][i_win+1]))\n",
    "        # Count the trajectories that passed through both box i during window i_win, and box j during window i_win+1. \n",
    "        # Maybe some trajectories will be double counted. \n",
    "        idx_pre = np.where(szn_window[src].data==i_win)\n",
    "        idx_post = np.where(szn_window[src].data==i_win+1)\n",
    "        overlap = np.where(\n",
    "            np.all(\n",
    "                np.array([\n",
    "                    (np.subtract.outer(idx_pre[dim], idx_post[dim]) == 0) \n",
    "                    for dim in nontime_dims\n",
    "                ]), axis=0\n",
    "            ) * (\n",
    "                np.subtract.outer(\n",
    "                    szn_start_year[src].data[idx_pre], szn_start_year[src].data[idx_post]\n",
    "                ) == 0\n",
    "            )          \n",
    "        )\n",
    "        idx_pre_overlap = tuple([idx_pre[dim][overlap[0]] for dim in range(len(idx_pre))])\n",
    "        idx_post_overlap = tuple([idx_post[dim][overlap[1]] for dim in range(len(idx_pre))])\n",
    "        km_pre = km_assignment[src].data[idx_pre_overlap]\n",
    "        km_post = km_assignment[src].data[idx_post_overlap]\n",
    "        ensemble_member_year_identifier = np.concatenate((\n",
    "            np.array(idx_pre_overlap)[nontime_dims,:], \n",
    "            [szn_start_year[src].data[idx_pre_overlap]]\n",
    "        ), axis=0)\n",
    "        for i in range(P.shape[0]):\n",
    "            for j in range(P.shape[1]):\n",
    "                traj_idx, = np.where((km_pre==i)*(km_post==j))\n",
    "                P[i,j] = np.unique(ensemble_member_year_identifier[:,traj_idx], axis=1).shape[1]\n",
    "        min_rowsum = np.min(np.sum(P, axis=1))\n",
    "        min_colsum = np.min(np.sum(P, axis=0))\n",
    "        if min_rowsum == 0 or min_colsum == 0:\n",
    "            raise Exception(f\"Under-filled transition matrices between seasonal windows {i_win} and {i_win+1}. min_rowsum = {min_rowsum} and min_colsum = {min_colsum}\")\n",
    "        P_list[src] += [P]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each matrix\n",
    "for src in [\"hc\",\"ra\"]:\n",
    "    for i_win in range(szn_stats.attrs[\"Nt_szn\"]-1):\n",
    "        P_list[src][i_win] = (\n",
    "            np.diag(1.0/np.sum(P_list[src][i_win], axis=1))\n",
    "            .dot(P_list[src][i_win])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the data relevant for clustering\n",
    "msm_info = dict()\n",
    "for src in [\"hc\",\"ra\"]:\n",
    "    msm_info[src] = dict({\n",
    "        \"szn_window\": szn_window[src],\n",
    "        \"szn_start_year\": szn_start_year[src],\n",
    "        \"traj_beginning_flag\": traj_beginning_flag[src],\n",
    "        \"traj_ending_flag\": traj_ending_flag[src],\n",
    "        \"km_centers\": km_centers[src],\n",
    "        \"km_assignment\": km_assignment[src],\n",
    "        \"km_n_clusters\": km_n_clusters[src],\n",
    "        \"P_list\": P_list[src],\n",
    "    })\n",
    "pickle.dump(msm_info, open(join(filedict[\"results\"][\"dir\"], \"msm_info\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msm_info = pickle.load(open(join(filedict[\"results\"][\"dir\"], \"msm_info\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to assign every point in the dataset a value according to its K-means assignment\n",
    "def broadcast_field_msm2dataarray(msm, field_msm, szn_stats, density_flag=False):\n",
    "    field_da = np.zeros(msm[\"szn_window\"].shape)\n",
    "    for i_win in range(szn_stats.attrs[\"Nt_szn\"]):\n",
    "        idx_in_window = np.where(msm[\"szn_window\"].data == i_win)\n",
    "        for i_clust in range(msm[\"km_n_clusters\"][i_win]):\n",
    "            idx_in_cluster = np.where(msm[\"km_assignment\"].data[idx_in_window] == i_clust)\n",
    "            idx_in_window_and_cluster = tuple([idx_in_window[dim][idx_in_cluster] for dim in range(len(idx_in_window))])\n",
    "            field_da[idx_in_window_and_cluster] = field_msm[i_win][i_clust]\n",
    "            if density_flag and (len(idx_in_window_and_cluster[0]) > 0):\n",
    "                field_da[idx_in_window_and_cluster] *= 1.0/len(idx_in_window_and_cluster[0])\n",
    "    da_broadcast = xr.DataArray(\n",
    "        coords = msm[\"szn_window\"].coords,\n",
    "        dims = msm[\"szn_window\"].dims, \n",
    "        data = field_da,\n",
    "    )\n",
    "    return da_broadcast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for the committor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First assign flags to each cluster center for A or B membership. For this we need to refer to the original data in the membership list of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of vectors to flag whether each cluster at each time is in A or is in B\n",
    "src = \"hc\"\n",
    "ina = []\n",
    "inb = []\n",
    "for i_win in range(szn_stats.attrs[\"Nt_szn\"]):\n",
    "    ina += [np.zeros(km_n_clusters[src][i_win], dtype=float)]    \n",
    "    inb += [np.zeros(km_n_clusters[src][i_win], dtype=float)]\n",
    "    idx_in_window = np.where(szn_window[src].data==i_win)\n",
    "    ab_tag_in_window = ab_tag[src].data[idx_in_window]\n",
    "    for i_clust in range(km_n_clusters[src][i_win]):\n",
    "        idx_in_cluster = np.where(km_assignment[src].data[idx_in_window]==i_clust)\n",
    "        ina[i_win][i_clust] = 1.0*(np.mean(ab_tag_in_window[idx_in_cluster]==abcode[\"A\"]) == 1.0)\n",
    "        inb[i_win][i_clust] = 1.0*(np.mean(ab_tag_in_window[idx_in_cluster]==abcode[\"B\"]) == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.concatenate(inb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the time-dependent Markov Chain class\n",
    "import tdmc_obj\n",
    "mc = tdmc_obj.TimeDependentMarkovChain(msm_info[src][\"P_list\"], szn_stats.attrs[\"t_szn_cent\"])\n",
    "\n",
    "# Solve for the committor\n",
    "G = [] \n",
    "F = [] \n",
    "for i in range(mc.Nt):\n",
    "    G += [1.0*inb[i]]\n",
    "    if i < mc.Nt-1: F += [1.0*np.outer((ina[i]==0)*(inb[i]==0), np.ones(mc.Nx[i+1]))]\n",
    "qp = mc.dynamical_galerkin_approximation(F,G)\n",
    "\n",
    "# Solve for the time-dependent density\n",
    "init_dens = np.ones(msm_info[\"hc\"][\"km_n_clusters\"][0]) \n",
    "init_dens *= 1.0/np.sum(init_dens)\n",
    "dens = mc.propagate_density_forward(init_dens)\n",
    "\n",
    "# Solve for the backward committor\n",
    "P_list_bwd = []                                                        \n",
    "for i in np.arange(mc.Nt-2,-1,-1):\n",
    "    P_list_bwd += [(msm_info[\"hc\"][\"P_list\"][i] * np.outer(dens[i], 1.0/dens[i+1])).T]        \n",
    "    rowsums = np.sum(P_list_bwd[-1],axis=1)                    \n",
    "G = []\n",
    "F = []\n",
    "for i in np.arange(mc.Nt-1,-1,-1):                             \n",
    "    G += [1.0*ina[i]]                                          \n",
    "    if i < mc.Nt-1: \n",
    "        Fnew = np.outer(1.0*(ina[i+1]==0)*(inb[i+1]==0), np.ones(len(inb[i]))) \n",
    "        F += [Fnew.copy()]\n",
    "qm = mc.dynamical_galerkin_approximation(F,G)                  \n",
    "qm.reverse()\n",
    "\n",
    "# Solve for the rate\n",
    "flux = []\n",
    "rate_froma = 0\n",
    "rate_tob = 0\n",
    "flux_froma = []\n",
    "flux_tob = []\n",
    "flux_dens_tob = np.zeros(szn_stats.attrs[\"Nt_szn\"])\n",
    "for ti in range(szn_stats.attrs[\"Nt_szn\"]-1):\n",
    "    flux += [(msm_info[\"hc\"][\"P_list\"][ti].T * dens[ti] * qm[ti]).T * qp[ti+1]]\n",
    "    flux_froma += [(msm_info[\"hc\"][\"P_list\"][ti].T * dens[ti] * ina[ti]).T * qp[ti+1]]\n",
    "    flux_tob += [(msm_info[\"hc\"][\"P_list\"][ti].T * dens[ti] * qm[ti]).T * inb[ti+1]]\n",
    "    rate_froma += np.sum(flux_froma[-1])\n",
    "    rate_tob += np.sum(flux_tob[-1])\n",
    "    flux_dens_tob[ti] = np.sum(flux_tob[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_froma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_da = broadcast_field_msm2dataarray(msm_info[src], qp, szn_stats)\n",
    "pi_da = broadcast_field_msm2dataarray(msm_info[src], dens, szn_stats, density_flag=True)\n",
    "qm_da = broadcast_field_msm2dataarray(msm_info[src], qm, szn_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(ncols=3,figsize=(15,5),sharey=True)\n",
    "cond = (\n",
    "    (feat_tpt[\"hc\"][\"X\"].sel(feature=\"t_szn\",drop=True) > tpt_bndy[\"tthresh\"][0]) * \n",
    "    (feat_tpt[\"hc\"][\"X\"].sel(feature=\"t_szn\",drop=True) < tpt_bndy[\"tthresh\"][1]) \n",
    ")\n",
    "feat0,feat1 = \"t_szn\",\"x1\"\n",
    "features = np.array([feat_tpt[\"hc\"][\"X\"].sel(feature=ft).where(cond).data.flatten() for ft in [feat0,feat1]]).T\n",
    "weights = np.ones(features.shape[0])\n",
    "helper2.plot_field_2d(\n",
    "    qp_da.where(cond).data.flatten(),\n",
    "    weights,\n",
    "    features,\n",
    "    feat_names = [feat0,feat1], \n",
    "    shp=[25,25], fig=fig, ax=axes[0], field_name=r\"$q_B^+$\"\n",
    ")\n",
    "helper2.plot_field_2d(\n",
    "    pi_da.where(cond).data.flatten(),\n",
    "    weights,\n",
    "    features,\n",
    "    feat_names = [feat0,feat1], \n",
    "    shp=[25,25], fig=fig, ax=axes[1], field_name=r\"$\\pi$\",\n",
    "    stat_name=\"sum\"\n",
    ")\n",
    "helper2.plot_field_2d(\n",
    "    qm_da.where(cond).data.flatten(),\n",
    "    weights,\n",
    "    features,\n",
    "    feat_names = [feat0,feat1], \n",
    "    shp=[25,25], fig=fig, ax=axes[2], field_name=r\"$q_A^-$\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
